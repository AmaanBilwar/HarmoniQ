<!-- ### havent decided on a name yet so for now welcome to 

# MUSIC GENERATION PROJECT test01 (100% Working real)


## Inspiration

This app bridges the communication gap for children with developmental disabilities—an estimated 1 in 10 in the United States—by using facial expression recognition to turn emotions into music, creating a personalized, immersive musical experience.


<!-- add why i love music and would like to help -->


1. Identifying and adapting resources for non-verbal and special needs students.
2. Integrating music therapy principles into lessons to foster creativity.
3. Encouraging improvisation to facilitate emotional expression.
4. Navigating the complexities of individual accessibility needs.

Despite these efforts, many children are still left without accessible tools for self-expression. That’s where my project, steps in. By leveraging facial expression recognition, we can enable children to communicate through music using their natural facial expressions, providing a more intuitive and inclusive approach to creativity. Through my project, I am committed to making music a universal language, empowering every child—and anyone facing communication challenges—to express their true selves.

## The PROBLEM


## The IDEA🧠

## rough how it works (potentially)
- clicks
2. clicks pictures every 10 / 30 seconds 
3. feeds those pictures to nvidia llama3.2 90B vision model to generate prompts
4. prompts are use with suno api to generate music

## Rough architecture


## Implementation

## Resources used & WHY❓ 

1. [Suno App](https://suno.com) -> where this idea came from?
    - [my SumoAPI hosted on vercel](https://suno-nr7ojy5e4-amaanbilwars-projects.vercel.app/)
2. [Suno Api](https://github.com/gcui-art/suno-api) 
3. [Alternative to Suno](https://github.com/facebookresearch/audiocraft)
4. [Image Analysis](http://build.nvidia.com/)
4. [Cohere](https://cohere.com/)
5. [anthropic](https://console.anthropic.com/)
-->